{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLkCmRCJF0uD",
        "outputId": "af02e09d-623e-4f68-bc33-762d0b77aaf6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.8.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EcI4CqRUDALx"
      },
      "outputs": [],
      "source": [
        "class DataLoaderLite:\n",
        "    def __init__(self, B, T):\n",
        "        self.B = B  # batch size\n",
        "        self.T = T  # sequence length\n",
        "\n",
        "        # Load tokens from disk and store them in memory\n",
        "        with open('input.txt', 'r') as f:\n",
        "            text = f.read()\n",
        "\n",
        "        # Initialize tokenizer\n",
        "        enc = tiktoken.get_encoding('gpt2')\n",
        "        tokens = enc.encode(text)\n",
        "        self.tokens = torch.tensor(tokens)\n",
        "        print(f'Loaded {len(self.tokens):,} tokens')\n",
        "        print(f'1 epoch = {len(self.tokens) // (B * T):,} batches')\n",
        "\n",
        "        # Initialize position\n",
        "        self.current_position = 0\n",
        "\n",
        "    def next_batch(self):\n",
        "        B, T = self.B, self.T\n",
        "\n",
        "        # Get the next batch of tokens\n",
        "        buf = self.tokens[self.current_position:self.current_position + B*T + 1]\n",
        "\n",
        "        # Reset position if we're at the end\n",
        "        if len(buf) < B*T + 1:\n",
        "            self.current_position = 0\n",
        "            buf = self.tokens[:B*T + 1]\n",
        "\n",
        "        # Create input and target tensors\n",
        "        x = buf[:-1].view(B, T)\n",
        "        y = buf[1:].view(B, T)\n",
        "\n",
        "        # Update position\n",
        "        self.current_position += B*T\n",
        "\n",
        "        return x, y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kAB4yXcIDnTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import time\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import tiktoken"
      ],
      "metadata": {
        "id": "TICFB6vnDjO0"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANGPT_SCALE_INIT = 1\n",
        "        # regularization\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = config.dropout\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
        "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        # att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
        "        # att = F.softmax(att, dim=-1)\n",
        "        # y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "\n",
        "        y = F.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0.0, is_causal=True) # Flash attention\n",
        "\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "        # output projection\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n"
      ],
      "metadata": {
        "id": "uVTePJ1iDXES"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu    = nn.GELU(approximate='tanh')\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "7UUZ4G_uD7Pz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "vJxosWJcD-Ci"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024 # max sequence length\n",
        "    vocab_size: int = 50304 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
        "    n_layer: int = 12 # number of layers\n",
        "    n_head: int = 12 # number of heads\n",
        "    n_embd: int = 768 # embedding dimension\n",
        "    dropout: float = 0.1  # Added dropout parameter\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # weight sharing\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        # weight initialization\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            std = 0.02\n",
        "            if hasattr(module, 'NANGPT_SCALE_INIT'):\n",
        "                std *= (2 * self.config.n_layer) ** -0.5\n",
        "            torch.nn.init.normal_(module.weight, mean = 0.0, std = std)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std = 0.02)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx is of shape (B, T)\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
        "        # forward the token and posisition embeddings\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device) # shape (T)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
        "        x = tok_emb + pos_emb\n",
        "        # forward the blocks of the transformer\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        # forward the final layernorm and the classifier\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, device_type):\n",
        "        # start with all of the candidate parameters (that require grad)\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == \"cuda\"\n",
        "\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
        "        return optimizer\n"
      ],
      "metadata": {
        "id": "dI7txeiGEDVc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "print(f\"using device: {device}\")\n",
        "\n",
        "# Optimize training configuration for Colab\n",
        "def get_training_config(device):\n",
        "    if device == 'cuda':  # Colab GPU configuration\n",
        "        block_size = 512  # Reduced from 1024 for Colab memory\n",
        "        return {\n",
        "            'batch_size': 8,            # Reduced batch size for Colab GPU\n",
        "            'seq_length': block_size,\n",
        "            'max_lr': 3e-4,             # Slightly reduced learning rate\n",
        "            'min_lr': 1e-5,\n",
        "            'warmup_steps': 1000,\n",
        "            'max_steps': 100000,\n",
        "            'gradient_accumulation_steps': 4,  # Increased for effective batch size\n",
        "            'log_interval': 50,\n",
        "            'model_config': GPTConfig(\n",
        "                block_size=block_size,\n",
        "                n_layer=8,              # Reduced from 12\n",
        "                n_head=8,              # Reduced from 12\n",
        "                n_embd=512,            # Reduced from 768\n",
        "                dropout=0.1\n",
        "            )\n",
        "        }\n",
        "    elif device == 'mps':\n",
        "        return {\n",
        "            'batch_size': 4,\n",
        "            'seq_length': 128,\n",
        "            'max_lr': 5e-4,\n",
        "            'min_lr': 1e-5,\n",
        "            'warmup_steps': 1000,\n",
        "            'max_steps': 100000,\n",
        "            'gradient_accumulation_steps': 8,\n",
        "            'log_interval': 50,\n",
        "            'model_config': GPTConfig(\n",
        "                block_size=256,\n",
        "                n_layer=6,\n",
        "                n_head=8,\n",
        "                n_embd=384,\n",
        "                dropout=0.1\n",
        "            )\n",
        "        }\n",
        "    else:  # CPU configuration\n",
        "        return {\n",
        "            'batch_size': 4,\n",
        "            'seq_length': 256,\n",
        "            'max_lr': 1e-4,\n",
        "            'min_lr': 1e-5,\n",
        "            'warmup_steps': 1000,\n",
        "            'max_steps': 100000,\n",
        "            'gradient_accumulation_steps': 8,\n",
        "            'log_interval': 50,\n",
        "            'model_config': GPTConfig(\n",
        "                block_size=256,\n",
        "                n_layer=6,\n",
        "                n_head=6,\n",
        "                n_embd=384,\n",
        "                dropout=0.1\n",
        "            )\n",
        "        }\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBS5GBQkEF3d",
        "outputId": "2d9a6122-9ab0-44da-f6ed-85ad798bf557"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimized learning rate scheduler\n",
        "class CosineWarmupScheduler:\n",
        "    def __init__(self, max_lr, min_lr, warmup_steps, max_steps):\n",
        "        self.max_lr = max_lr\n",
        "        self.min_lr = min_lr\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.max_steps = max_steps\n",
        "\n",
        "    def get_lr(self, step):\n",
        "        if step < self.warmup_steps:\n",
        "            return self.max_lr * (step + 1) / self.warmup_steps\n",
        "        if step > self.max_steps:\n",
        "            return self.min_lr\n",
        "        decay_ratio = (step - self.warmup_steps) / (self.max_steps - self.warmup_steps)\n",
        "        coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "        return self.min_lr + coeff * (self.max_lr - self.min_lr)\n",
        "\n",
        "# Add model saving function\n",
        "def save_checkpoint(model, optimizer, config, step, loss, best_loss, save_path):\n",
        "    checkpoint = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'config': config,\n",
        "        'step': step,\n",
        "        'loss': loss,\n",
        "        'best_loss': best_loss\n",
        "    }\n",
        "    torch.save(checkpoint, save_path)\n",
        "    #print(f\"Checkpoint saved: {save_path}\")"
      ],
      "metadata": {
        "id": "M9fIUiJjEVuR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simplify train_model function\n",
        "def train_model(model, train_loader, config, device):\n",
        "    optimizer = model.configure_optimizers(\n",
        "        weight_decay=0.1,\n",
        "        learning_rate=config['max_lr'],\n",
        "        device_type=device\n",
        "    )\n",
        "\n",
        "    scheduler = CosineWarmupScheduler(\n",
        "        max_lr=config['max_lr'],\n",
        "        min_lr=config['min_lr'],\n",
        "        warmup_steps=config['warmup_steps'],\n",
        "        max_steps=config['max_steps']\n",
        "    )\n",
        "\n",
        "    model.train()\n",
        "    total_tokens = 0\n",
        "    best_loss = float('inf')\n",
        "\n",
        "    # Create checkpoints directory if it doesn't exist\n",
        "    os.makedirs('checkpoints', exist_ok=True)\n",
        "\n",
        "    print(\"\\n=== Starting Training ===\")\n",
        "    print(f\"Training for {config['max_steps']:,} steps\")\n",
        "    print(f\"Logging every {config['log_interval']} steps\\n\")\n",
        "\n",
        "    for step in range(config['max_steps']):\n",
        "        t0 = time.time()\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        accumulated_loss = 0\n",
        "\n",
        "        for micro_step in range(config['gradient_accumulation_steps']):\n",
        "            x, y = train_loader.next_batch()\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            if device == 'mps':\n",
        "                logits, loss = model(x, y)\n",
        "            else:\n",
        "                with torch.autocast(device_type=device, dtype=torch.bfloat16):\n",
        "                    logits, loss = model(x, y)\n",
        "\n",
        "            loss = loss / config['gradient_accumulation_steps']\n",
        "            loss.backward()\n",
        "            accumulated_loss += float(loss.detach().cpu().item())\n",
        "\n",
        "            del logits, loss\n",
        "            if device == 'mps':\n",
        "                torch.mps.empty_cache()\n",
        "\n",
        "        # Gradient clipping\n",
        "        norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Learning rate update\n",
        "        lr = scheduler.get_lr(step)\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        # Modified logging frequency\n",
        "        if step % config['log_interval'] == 0:\n",
        "            elapsed = time.time() - t0\n",
        "            print(f\"Step {step:6d}/{config['max_steps']:6d} | \"\n",
        "                  f\"Loss: {accumulated_loss:.4f} | \"\n",
        "                  f\"LR: {lr:.2e} | \"\n",
        "                  f\"Best: {best_loss:.4f} | \"\n",
        "                  f\"Time: {elapsed:.2f}s\")\n",
        "\n",
        "        # Track best loss\n",
        "        if accumulated_loss < best_loss:\n",
        "            best_loss = accumulated_loss\n",
        "\n",
        "        if accumulated_loss < 0.099999:\n",
        "            print(f\"\\n🎉 Target loss achieved at step {step:,}!\")\n",
        "            save_checkpoint(\n",
        "                model=model,\n",
        "                optimizer=optimizer,\n",
        "                config=config,\n",
        "                step=step,\n",
        "                loss=accumulated_loss,\n",
        "                best_loss=best_loss,\n",
        "                save_path=f'checkpoints/target_achieved_model.pt'\n",
        "            )\n",
        "            break\n",
        "\n",
        "    print(\"\\n=== Training Complete ===\")\n",
        "    print(f\"Best Loss: {best_loss:.6f}\")\n",
        "    print(f\"Total Steps: {step + 1:,}\")\n",
        "    return model\n",
        "\n",
        "# Add load checkpoint function\n",
        "def load_checkpoint(path, device):\n",
        "    checkpoint = torch.load(path, map_location=device)\n",
        "    config = checkpoint['config']\n",
        "    model = GPT(config['model_config']).to(device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    optimizer = model.configure_optimizers(\n",
        "        weight_decay=0.1,\n",
        "        learning_rate=config['max_lr'],\n",
        "        device_type=device\n",
        "    )\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    return model, optimizer, config, checkpoint['step'], checkpoint['loss'], checkpoint['best_loss']\n",
        "\n",
        "# Add detailed logging function\n",
        "def log_config(config, model, device):\n",
        "    print(\"\\n=== Training Configuration ===\")\n",
        "    print(f\"Device: {device}\")\n",
        "    print(\"\\nModel Architecture:\")\n",
        "    print(f\"- Layers: {config['model_config'].n_layer}\")\n",
        "    print(f\"- Heads: {config['model_config'].n_head}\")\n",
        "    print(f\"- Embedding Dim: {config['model_config'].n_embd}\")\n",
        "    print(f\"- Block Size: {config['model_config'].block_size}\")\n",
        "    print(f\"- Vocab Size: {config['model_config'].vocab_size}\")\n",
        "\n",
        "    print(\"\\nTraining Parameters:\")\n",
        "    print(f\"- Batch Size: {config['batch_size']}\")\n",
        "    print(f\"- Sequence Length: {config['seq_length']}\")\n",
        "    print(f\"- Gradient Accumulation Steps: {config['gradient_accumulation_steps']}\")\n",
        "    print(f\"- Effective Batch Size: {config['batch_size'] * config['gradient_accumulation_steps']}\")\n",
        "    print(f\"- Learning Rate: {config['max_lr']}\")\n",
        "\n",
        "    # Calculate model size\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    param_size = sum(p.nelement() * p.element_size() for p in model.parameters())\n",
        "    buffer_size = sum(b.nelement() * b.element_size() for b in model.buffers())\n",
        "    total_size = param_size + buffer_size\n",
        "\n",
        "    print(\"\\nModel Statistics:\")\n",
        "    print(f\"- Total Parameters: {total_params:,}\")\n",
        "    print(f\"- Trainable Parameters: {trainable_params:,}\")\n",
        "    print(f\"- Model Size: {total_size/1024/1024:.2f} MB\")\n",
        "    print(\"=\"*30 + \"\\n\")"
      ],
      "metadata": {
        "id": "IXbycBaQEdKJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rbqr70tPIjyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Get configuration and initialize model\n",
        "    config = get_training_config(device)\n",
        "    model = GPT(config['model_config']).to(device)\n",
        "\n",
        "    # Log detailed configuration\n",
        "    log_config(config, model, device)\n",
        "\n",
        "    # Initialize data loader\n",
        "    train_loader = DataLoaderLite(\n",
        "        B=config['batch_size'],\n",
        "        T=config['seq_length']\n",
        "    )\n",
        "\n",
        "    # Train model\n",
        "    torch.set_float32_matmul_precision('high')\n",
        "    model = train_model(model, train_loader, config, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIUKTQnGFlXU",
        "outputId": "358497bc-e0db-46b2-d598-c6f1b8a862ab"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Training Configuration ===\n",
            "Device: cuda\n",
            "\n",
            "Model Architecture:\n",
            "- Layers: 8\n",
            "- Heads: 8\n",
            "- Embedding Dim: 512\n",
            "- Block Size: 512\n",
            "- Vocab Size: 50304\n",
            "\n",
            "Training Parameters:\n",
            "- Batch Size: 8\n",
            "- Sequence Length: 512\n",
            "- Gradient Accumulation Steps: 4\n",
            "- Effective Batch Size: 32\n",
            "- Learning Rate: 0.0003\n",
            "\n",
            "Model Statistics:\n",
            "- Total Parameters: 51,237,888\n",
            "- Trainable Parameters: 51,237,888\n",
            "- Model Size: 203.46 MB\n",
            "==============================\n",
            "\n",
            "Loaded 338,025 tokens\n",
            "1 epoch = 82 batches\n",
            "num decayed parameter tensors: 34, with 51,183,616 parameters\n",
            "num non-decayed parameter tensors: 66, with 54,272 parameters\n",
            "using fused AdamW: True\n",
            "\n",
            "=== Starting Training ===\n",
            "Training for 100,000 steps\n",
            "Logging every 50 steps\n",
            "\n",
            "Step      0/100000 | Loss: 10.9322 | LR: 3.00e-07 | Best: inf | Time: 3.96s\n",
            "Step     50/100000 | Loss: 9.3276 | LR: 1.53e-05 | Best: 9.3480 | Time: 2.35s\n",
            "Step    100/100000 | Loss: 8.3049 | LR: 3.03e-05 | Best: 8.3672 | Time: 2.41s\n",
            "Step    150/100000 | Loss: 7.3769 | LR: 4.53e-05 | Best: 7.1525 | Time: 2.42s\n",
            "Step    200/100000 | Loss: 6.1051 | LR: 6.03e-05 | Best: 6.1665 | Time: 2.42s\n",
            "Step    250/100000 | Loss: 5.3377 | LR: 7.53e-05 | Best: 5.4109 | Time: 2.42s\n",
            "Step    300/100000 | Loss: 5.2891 | LR: 9.03e-05 | Best: 4.9928 | Time: 2.43s\n",
            "Step    350/100000 | Loss: 4.7174 | LR: 1.05e-04 | Best: 4.6147 | Time: 2.43s\n",
            "Step    400/100000 | Loss: 4.6163 | LR: 1.20e-04 | Best: 4.3609 | Time: 2.43s\n",
            "Step    450/100000 | Loss: 4.3081 | LR: 1.35e-04 | Best: 4.1965 | Time: 2.43s\n",
            "Step    500/100000 | Loss: 4.1621 | LR: 1.50e-04 | Best: 3.9128 | Time: 2.43s\n",
            "Step    550/100000 | Loss: 3.8806 | LR: 1.65e-04 | Best: 3.7974 | Time: 2.43s\n",
            "Step    600/100000 | Loss: 3.8613 | LR: 1.80e-04 | Best: 3.4816 | Time: 2.43s\n",
            "Step    650/100000 | Loss: 3.6470 | LR: 1.95e-04 | Best: 3.3251 | Time: 2.43s\n",
            "Step    700/100000 | Loss: 3.2434 | LR: 2.10e-04 | Best: 3.1647 | Time: 2.43s\n",
            "Step    750/100000 | Loss: 2.9828 | LR: 2.25e-04 | Best: 3.0222 | Time: 2.43s\n",
            "Step    800/100000 | Loss: 2.9813 | LR: 2.40e-04 | Best: 2.6765 | Time: 2.44s\n",
            "Step    850/100000 | Loss: 2.5895 | LR: 2.55e-04 | Best: 2.5286 | Time: 2.43s\n",
            "Step    900/100000 | Loss: 2.3633 | LR: 2.70e-04 | Best: 2.3610 | Time: 2.42s\n",
            "Step    950/100000 | Loss: 2.1342 | LR: 2.85e-04 | Best: 2.1786 | Time: 2.43s\n",
            "Step   1000/100000 | Loss: 2.0675 | LR: 3.00e-04 | Best: 1.9328 | Time: 2.43s\n",
            "Step   1050/100000 | Loss: 1.7522 | LR: 3.00e-04 | Best: 1.6885 | Time: 2.44s\n",
            "Step   1100/100000 | Loss: 1.8402 | LR: 3.00e-04 | Best: 1.4897 | Time: 2.44s\n",
            "Step   1150/100000 | Loss: 1.4111 | LR: 3.00e-04 | Best: 1.2847 | Time: 2.43s\n",
            "Step   1200/100000 | Loss: 1.0943 | LR: 3.00e-04 | Best: 1.0577 | Time: 2.43s\n",
            "Step   1250/100000 | Loss: 1.1707 | LR: 3.00e-04 | Best: 0.8989 | Time: 2.43s\n",
            "Step   1300/100000 | Loss: 0.7859 | LR: 3.00e-04 | Best: 0.7270 | Time: 2.43s\n",
            "Step   1350/100000 | Loss: 0.7085 | LR: 3.00e-04 | Best: 0.6097 | Time: 2.43s\n",
            "Step   1400/100000 | Loss: 0.5328 | LR: 3.00e-04 | Best: 0.5070 | Time: 2.44s\n",
            "Step   1450/100000 | Loss: 0.4833 | LR: 3.00e-04 | Best: 0.3867 | Time: 2.44s\n",
            "Step   1500/100000 | Loss: 0.3920 | LR: 3.00e-04 | Best: 0.3226 | Time: 2.43s\n",
            "Step   1550/100000 | Loss: 0.3018 | LR: 3.00e-04 | Best: 0.2456 | Time: 2.44s\n",
            "Step   1600/100000 | Loss: 0.3286 | LR: 3.00e-04 | Best: 0.1905 | Time: 2.43s\n",
            "Step   1650/100000 | Loss: 0.1852 | LR: 3.00e-04 | Best: 0.1746 | Time: 2.44s\n",
            "Step   1700/100000 | Loss: 0.1469 | LR: 3.00e-04 | Best: 0.1401 | Time: 2.45s\n",
            "Step   1750/100000 | Loss: 0.1319 | LR: 3.00e-04 | Best: 0.1222 | Time: 2.44s\n",
            "Step   1800/100000 | Loss: 0.1306 | LR: 3.00e-04 | Best: 0.1051 | Time: 2.43s\n",
            "\n",
            "🎉 Target loss achieved at step 1,832!\n",
            "\n",
            "=== Training Complete ===\n",
            "Best Loss: 0.099628\n",
            "Total Steps: 1,833\n"
          ]
        }
      ]
    }
  ]
}